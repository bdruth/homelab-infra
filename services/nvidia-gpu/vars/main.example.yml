---
# NVIDIA GPU Configuration Example Variables
# Copy this file to main.yml and customize as needed

# Enable/disable feature flags
nvidia_install_gpu_drivers: true # Whether to install NVIDIA GPU drivers
nvidia_install_cuda_toolkit: true # Whether to install CUDA toolkit
install_btop: true # Whether to install btop
install_ollama: true # Whether to install Ollama
install_uv: true # Whether to install uv Python package manager
install_n8n: true # Whether to install n8n AI workflow platform
install_whisper_asr: true # Whether to install Whisper ASR Webservice
install_open_webui: true # Whether to install Open WebUI
install_comfyui: true # Whether to install ComfyUI for image generation
install_shared_models: true # Whether to install shared models for multiple services
shared_models_backup_existing: true # Back up existing files before replacing with symlinks
install_docker_compose: true # Whether to install latest docker-compose
nvidia_configure_egpu: true # Whether to configure thunderbolt eGPU support
install_watchtower: true # Whether to install watchtower for automatic container updates
install_gpt_researcher: true # Whether to install GPT Researcher

# Package versions and configuration
nvidia_driver_version: "570" # Used in package name (nvidia-driver-570)
btop_version: "1.4.0" # Version to install from GitHub releases
n8n_version: "main" # Git branch or tag to use for n8n installation

# NVIDIA Container Runtime options
nvidia_container_debug: false # Set to true for debugging
nvidia_runtime_debug: false # Set to true for debugging
nvidia_load_kmods: true # Load kernel modules automatically
nvidia_no_cgroups: false # Set to true to disable cgroups support
nvidia_require_cuda: true # Require CUDA support

# n8n configuration
n8n_install_dir: "/opt/n8n-ai-starter-kit" # Installation directory
n8n_port: "5678" # Port for n8n web interface
n8n_protocol: "http" # Protocol for n8n web interface (http or https)
n8n_host: "localhost" # Host for n8n web interface
n8n_host_public: "localhost" # Public host for n8n web interface
n8n_webhook_url: "https://example.com/"

# n8n database configuration
n8n_postgres_user: "n8n" # PostgreSQL username
n8n_postgres_password: "n8n_password" # PostgreSQL password (change in production)
n8n_postgres_db: "n8n" # PostgreSQL database name
n8n_encryption_key: "12345678901234567890123456789012" # 32-character encryption key (change in production)

# Whisper ASR Webservice configuration
whisper_asr_install_dir: "/opt/whisper-asr" # Installation directory
whisper_asr_port: "9000" # Port for Whisper ASR API
whisper_asr_model: "large-v3" # Model size (tiny, base, small, medium, large, large-v3)
whisper_asr_engine: "faster_whisper" # Engine: faster_whisper or whisper
whisper_asr_compute_type: "{% if nvidia_install_gpu_drivers | bool %}float16{% else %}int8{% endif %}" # Compute type: float16 (GPU), int8 (CPU)

# Open WebUI configuration
open_webui_install_dir: "/opt/open-webui" # Installation directory
open_webui_port: "3000" # Port for Open WebUI
open_webui_enable_rag_web_search: true # Enable RAG web search
open_webui_enable_search_query_generation: true # Enable search query generation
open_webui_rag_web_search_engine: "duckduckgo" # Web search engine to use
open_webui_rag_web_loader_engine: "playwright" # Web loader engine to use
open_webui_enable_image_generation: true # Enable image generation integration

# ComfyUI configuration
comfyui_install_dir: "/opt/comfyui" # Installation directory
comfyui_port: "8188" # Port for ComfyUI
comfyui_user: "comfyui" # User to run ComfyUI
comfyui_group: "comfyui" # Group for ComfyUI user
comfyui_create_user: true # Whether to create a dedicated user for ComfyUI
comfyui_venv_path: "/opt/comfyui/venv" # Path to Python virtual environment
comfyui_git_repo: "https://github.com/comfyanonymous/ComfyUI.git" # Git repository
comfyui_git_branch: "master" # Git branch or tag
comfyui_torch_cuda_url: "https://download.pytorch.org/whl/cu121" # PyTorch CUDA wheels URL
comfyui_low_vram: true # Enable low VRAM mode (8GB VRAM or less)
comfyui_cpu_vae: true # Move VAE processing to CPU to reduce VRAM usage
comfyui_force_reinstall: false # Set to true to force removal and reinstallation

# ComfyUI model configuration
comfyui_install_flux1: true # Whether to install FLUX.1 models
comfyui_install_sd_v15: true # Whether to install Stable Diffusion v1.5 model
comfyui_skip_model_downloads: false # Skip downloading models (useful for testing)

# FLUX.1 model configuration
comfyui_flux1_variant: "schnell" # Which FLUX.1 variant to use (schnell or dev)
comfyui_flux1_model_url: >-
  https://huggingface.co/black-forest-labs/FLUX.1-{{ comfyui_flux1_variant }}/resolve/main/flux1-{{
  comfyui_flux1_variant }}.safetensors" # URL to the FLUX.1 model
comfyui_flux1_model_filename: "flux1-{{ comfyui_flux1_variant }}.safetensors" # Filename for the FLUX.1 model
comfyui_vae_url: "https://huggingface.co/black-forest-labs/FLUX.1-{{ comfyui_flux1_variant }}/resolve/main/ae.safetensors" # URL to the VAE model
comfyui_clip_url: "https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors" # URL to the CLIP model
comfyui_t5xxl_use_fp8: false # Use FP8 version (smaller file, less precise) instead of FP16
comfyui_t5xxl_url: >-
  https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/{% if comfyui_t5xxl_use_fp8
  %}t5xxl_fp8_e4m3fn.safetensors{% else %}t5xxl_fp16.safetensors{% endif %}" # URL to the T5XXL model
comfyui_t5xxl_filename: "{% if comfyui_t5xxl_use_fp8 %}t5xxl_fp8_e4m3fn.safetensors{% else %}t5xxl_fp16.safetensors{% endif %}" # Filename for the T5XXL model
# Stable Diffusion v1.5 model configuration
comfyui_sd_v15_url: "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors" # URL to the SD v1.5 model
comfyui_sd_v15_filename: "v1-5-pruned-emaonly.safetensors" # Filename for the SD v1.5 model

# GPT Researcher configuration
gpt_researcher_install_dir: "/opt/gpt-researcher" # Installation directory
gpt_researcher_port: "7000" # Port for GPT Researcher web interface
gpt_researcher_ollama_model: "llama3" # Ollama model to use for research
gpt_researcher_ollama_embed_model: "nomic-embed-text" # Ollama model to use for embeddings
